<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.104.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ju' Research and Life</title><meta name=description content><meta name=author content><link rel=canonical href=https://juyujeng.github.io/researchlife/><link crossorigin=anonymous href=/researchlife/assets/css/stylesheet.cb6161d21234b928031cf6aa795fd2bdad5c2483cefd0557b9eb0ed142628e25.css integrity="sha256-y2Fh0hI0uSgDHPaqeV/Sva1cJIPO/QVXuesO0UJijiU=" rel="preload stylesheet" as=style><link rel=icon href=https://juyujeng.github.io/researchlife/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://juyujeng.github.io/researchlife/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://juyujeng.github.io/researchlife/favicon-32x32.png><link rel=apple-touch-icon href=https://juyujeng.github.io/researchlife/apple-touch-icon.png><link rel=mask-icon href=https://juyujeng.github.io/researchlife/safari-pinned-tab.svg><link rel=manifest href=/site.webmanifest><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://juyujeng.github.io/researchlife/index.xml><link rel=alternate type=application/json href=https://juyujeng.github.io/researchlife/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-8XNMY2VF63"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8XNMY2VF63",{anonymize_ip:!1})}</script><meta property="og:title" content="Ju' Research and Life"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://juyujeng.github.io/researchlife/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ju' Research and Life"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Ju' Research and Life","url":"https://juyujeng.github.io/researchlife/","description":"","thumbnailUrl":"https://juyujeng.github.io/researchlife/favicon.ico","sameAs":[]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://juyujeng.github.io/researchlife/ accesskey=h title="Ju' Research & Life (Alt + H)"><img src=https://juyujeng.github.io/researchlife/apple-touch-icon.png alt aria-label=logo height=30>Ju' Research & Life</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://juyujeng.github.io/researchlife/about title="About Me"><span>About Me</span></a></li><li><a href=https://juyujeng.github.io/researchlife/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://juyujeng.github.io/researchlife/docs/ title=Document><span>Document</span></a></li><li><a href=https://juyujeng.github.io/researchlife/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://juyujeng.github.io/researchlife/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://juyujeng.github.io/researchlife/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>歡迎來到我的網站</h1></header><div class=entry-content><p>分享我的研究相關心得與經驗</p><ul><li>如果有興趣想要追求身心的平衡</li><li>可以參考 <a href=https://bmaa.tw>臺灣身心中軸覺察發展中心</a></li></ul></div><footer class=entry-footer><div class=social-icons></div></footer></article><article class=post-entry><header class=entry-header><h2>compare two binary diagnostic tests</h2></header><div class=entry-content><p>如何比較兩個診斷方式的準確性？ 幾個可以比較的指標 sensitivity (Se) = 病人有病且可以被診斷出有病的機率
specificity (Sp) = 病人沒病且可以被診斷沒病的機率
positive predicted value (PPV) = 被診斷為有病而實際上真的有病的機率
negative predicted value (NPV) = 被診斷為沒病而實際上真的沒病的機率
可以使用的方法 統計方法 比較的指標 McNemar test sensitivity、specificity GEE PPV、NPV weighted generalized score PPV、NPV 如何計算 GEE 的計算很難，而且在使用上的結果比較不直觀。weighted generalized score是近期比較多人使用的。而這個指標在計算上也是在不同的文獻當中有許多的改進。幸好最近有人將這些方法寫成了R的套件，有和DTComPair和compbdt兩個。
不過最近（2022/10/18）我發現DTComPair在新版的R已經不支援了，可能要找舊版的R去安裝，或是希望作者可以有更新。目前可能暫時就剩下compbdt可以使用。
compbdt的使用 compbdt其實不是一個完整的套件，只有一個函式，可以在原始論文的補充資料當中獲得。
使用時需要輸入兩種診斷工具的診斷結果（見下表）
compbdt(s11 = s11, s10 = s10, s01 = s01, s00 = s00, r11 = r11, r10 = r10, r01 = r01, r00 = r00, alpha = .05) 兩個診斷工具的診斷結果代號表（取自Roldán-Nofuentes, 2020） 這個函式的寫法會直接把分析的結果輸出到畫面上，並不會回傳任何資料。如果想要讓該函式回傳需要的統計值的話，需要自行修改作者提供的程式碼...</p></div><footer class=entry-footer><span title='2022-10-18 00:00:00 +0000 UTC'>October 18, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to compare two binary diagnostic tests" href=https://juyujeng.github.io/researchlife/docs/methods/compare-two-binary-diagnostic-tests/></a></article><article class=post-entry><header class=entry-header><h2>小時候看越多電視越容易自閉？</h2></header><div class=entry-content><p>日本的研究發現，一歲時越常看電視，在三歲前被診斷有自閉光譜疾患的機率較高。但這樣子的結果只在男童上觀察到，女童則無。</p></div><footer class=entry-footer><span title='2022-10-10 00:00:00 +0000 UTC'>October 10, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to 小時候看越多電視越容易自閉？" href=https://juyujeng.github.io/researchlife/docs/psychology/%E5%B0%8F%E6%99%82%E5%80%99%E7%9C%8B%E8%B6%8A%E5%A4%9A%E9%9B%BB%E8%A6%96%E8%B6%8A%E5%AE%B9%E6%98%93%E8%87%AA%E9%96%89/></a></article><article class=post-entry><header class=entry-header><h2>unitization problem and code reliability</h2></header><div class=entry-content><p>unitization problem 在進行文本的分析時通常需要先將文本轉編為可以分析的單位，而要如何決定文本當中的那一個段落是值得分析的過程就是unitization。以Empathic Communication Coding System (ECCS)的分析為例。如何決定哪一個溝通的橋段出現同理的時機，值得進一步去分析治療師同理回應能力就是unitization。
但是當有兩個以上的coders進行文本分析時，很有可能出現這些coders對同一個文本所辨識出來的分析單位（unit）對不上的情況，這就是一種unitization problem造成的後果。
有些文本的分析是直接以時間或是段落為單位，例如整段錄音（影）每30秒為一個單位，判斷這個單位內有沒有出現目標的行為。這種情況下就沒有什麼unitization problem。但有些文本的分析是以unit of meaning (Campbell et a., 2013）為單位，也就是要先事先定義好某些特定的目標，有出現才將該段落納入分析，像ECCS就是這樣子的例子。
那要怎麼解決不同的coders找到不同的units呢？Campbell等人（2013）提出一個方法，那就是讓研究的主持人（通常是對於該主題最瞭解的人）做為主要unit辨別者，其他的coders只在主持人所找到的units當中進行資料的分析以及分類。這樣子就只會有分類上的歧異，而不會有unitization problem。但大家都知道，主持人很忙，所以最少也要讓主持人主導訓練coders的任務，並且在分類規則上多加參與，以盡量減少unitization problem。
reliability of unitizing 目前針對unitization的過程進行信度或是一致性的指標似乎只有Guetzkow’s U。其他分類一致性的指標我覺得指標的目的都不全然符合。因為文本當中可能有一大堆片段都沒有被挑出來，這樣可以衝出許多不同coders都一致認為不算meaningful unit的資料點。因此只能從所有的coders都挑出來的units當中去計算信度。但這樣子畫出來的2 by 2 contingency table又會缺少雙方都認為不是的那一格。
Guetzkow’s U Guetzkow’s U則是針對已經找到的units數量上來計算兩個coders所找到的數量上是否一致。其公式如下：
$$U = \frac{O_1 - O_2}{O_1 + O_2}$$
其中$O_1$代表第一個coder找到的所有units數量，而$O_2$則是第二位coder找到的units數量。
這個方法假設，如果兩個coders的能力是相同的，那麼雙方找到的units數量的期望值應該會是相同的（為常數$h$）。
因此，$U$越接近0，應該兩個coders的unitizing越一致。
references Campbell, J. L., Quincy, C., Osserman, J., & Pedersen, O. K. (2013). Coding In-depth Semistructured Interviews: Problems of Unitization and Intercoder Reliability and Agreement. Sociological Methods & Research, 42(3), 294–320....</p></div><footer class=entry-footer><span title='2022-09-29 00:00:00 +0000 UTC'>September 29, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to unitization problem and code reliability" href=https://juyujeng.github.io/researchlife/docs/methods/unitization-problem-and-unitizing-reliability/></a></article><article class=post-entry><header class=entry-header><h2>順順的就過去了</h2></header><div class=entry-content><p>國小的某一次段考後，老師說改考卷時就是改到我的和某個同學的最好改，字大又漂亮。當時我心想，改考卷好不好改跟字漂不漂亮有什麼關係？後來研究所時當了助教，我才知道，字好看，真的是很有幫助的。不只是對批改者的閱讀性很有幫助，對於參與考試的人的成績也可能有幫助。
考卷上的字若好看，便容易閱讀，答案在哪裡、寫什麼，都一目了然。這樣子的批改的過程會讓批改者自己覺得流程非常的順暢。如果遇到了一些模稜兩可，不是全然正確的答案時，也很容易順順地就過去了。相反的，如果某張考卷上的字很難辨識，甚至排版雜亂時，就算答案正確，也可能因為看得頭昏眼花而改錯。若遇到了一些模稜兩可，不是全然正確的答案時，也常常因為辨識困難，又或者寫錯的小地方被放大檢視而不給分數。
因此，字若寫得好看，在某些可能得分也可能扣分的情況下，是很容易讓批改者更傾向給分數的。為什麼會有這種情況？這和人類的認知系統有關。
人性本懶的認知系統 人們的思考、行為主要受到兩種認知歷程所影響，直覺和深思。直覺是由經驗、習慣、本能而來，而深思則涉及抽象、去脈絡的思考。直覺的使用是不費力又快速的，而深思則是耗能又較為緩慢的。
人性本懶，能夠不動腦就不動腦。故大部份的情況底下人們都是憑著直覺反應在思考或是行動的，只有在某些時候人們願意多花一點心力思考時，深思的歷程才會介入。
那什麼時候人們願意多動點腦筋呢？目前有兩種看法。Stanovich（2010）認為這個和人的反思特質有關，反思特質高的人可能較為謹慎，較願意花心力來思考，因而較常啟動深思歷程，而不那麼常仰賴直覺（相對反思特質低的人而言，大致上來說，人們應還是比較常用直覺）。Thompson等人（2011）則認為人們對於直覺產生的思考或是行為結果會有一個是否正確的直覺感（feeling of rightness），如果感覺好像怪怪的，就會停下來深思一下；但如果感覺好像沒有什麼問題，那麼就不會再多花力氣。
這兩種看法間並沒有互相抵觸，我認為也許兩種皆有可能。直覺最常使用，但深思之後可以改變行為的結果。不過是否願意深思則會受到個人本身的特質或是對於當下第一時間的反應是否覺得怪怪的所影響。
人類思考歷程三系統間關係示意圖 為什麼字好看很容易讓批改者更傾向給分數？ 知道上述的認知系統之後就比較好理解了。當我們要批評他人，針對他人吹毛求疪時需要動用到費力的深思歷程。所以如果在批改考卷的過程中，讓批改者的深思歷程不要那麼常啟動，那麼在遇到模稜兩可的答案時，也就比較不會去深辨到底答案是對還是錯。而在批改過程中，那種「順順的」體感，便會減少深思啟動的機率。但如果批改過程卡卡的，很容易會讓批改者較容易覺得怪怪的，而讓啟動深思歷程的機率提升。
報告、演講的應用 同樣的道理，在進行口頭報告時，雖然說內容是否言之有物更重要，但投影片的排版、演講者的口條這些也會影響到聽眾聆聽過程的體感。投影片看起來不順眼、講話卡卡不流暢，都會讓聽眾無法「順順的」聽過去。這時聽眾不是覺得自己腦袋有問題，那麼就是講者講得有問題啦！
references Stanovich, K. (2010). The Tri-process model. In Rationality and the Reflective Mind. Oxford University Press. https://doi.org/10.1093/acprof:oso/9780195341140.001.0001
Thompson, V. A., Prowse Turner, J. A., & Pennycook, G. (2011). Intuition, reason, and metacognition. Cognitive Psychology, 63(3), 107–140. https://doi.org/10.1016/j.cogpsych.2011.06.001</p></div><footer class=entry-footer><span title='2022-09-28 00:00:00 +0000 UTC'>September 28, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to 順順的就過去了" href=https://juyujeng.github.io/researchlife/blog/life/%E9%A0%86%E9%A0%86%E7%9A%84%E5%B0%B1%E9%81%8E%E5%8E%BB%E4%BA%86/></a></article><article class=post-entry><header class=entry-header><h2>firefox真的比較隱密</h2></header><div class=entry-content><p>在網頁上啟用了google analytic，但試了半天發現都沒有任何資料進來
後來靈機一動把原本用的firefox換成google chrome
結果馬上就有瀏灠資料了
firefox的告訴網站不要追蹤功能還真的有效！？
經朋友提點，原來firefox在2019年年中之後預設會擋google analytic</p></div><footer class=entry-footer><span title='2022-09-27 00:00:00 +0000 UTC'>September 27, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to firefox真的比較隱密" href=https://juyujeng.github.io/researchlife/blog/life/firefox%E7%9C%9F%E7%9A%84%E6%AF%94%E8%BC%83%E9%9A%B1%E5%AF%86/></a></article><article class=post-entry><header class=entry-header><h2>應該要幫助他嗎</h2></header><div class=entry-content><p>白目的例子 家裡附近住著一位盲胞，在倒垃圾的時間很常見到他提著垃圾袋出來要倒垃圾。
許多時候，住在附近的鄰居或是也要倒垃圾的路人看到他，都會主動上前說要幫他倒垃圾。
有幾次，我出門倒垃圾時恰好看到他一個人提著垃圾袋走在我前面。我一直在想，他會不會有想要自己完成這種生活雜事的心態呢？所以我總是跟在後面，都沒有上前說要幫忙。
但每一次，就算已經快到垃圾車附近，總是會有倒垃圾的路人一個箭步上前向他拿過他手上的垃圾袋。我也一直沒有看到，他成功將垃圾袋丟上垃圾車的那一幕。
到底他有沒有想要自己完成這種生活雜事的心態呢？這個問題其實我只要在一見到他時就問他是否需要幫忙，而不是直接說我幫你倒垃圾，也許就可以知道了吧？只是我一直懶得去問而已。
而每次看來都是欣然接受幫忙的他，或許也沒有那麼強烈想要自己完成的願望吧。</p></div><footer class=entry-footer><span title='2022-09-24 00:00:00 +0000 UTC'>September 24, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to 應該要幫助他嗎" href=https://juyujeng.github.io/researchlife/blog/life/%E6%87%89%E8%A9%B2%E8%A6%81%E5%B9%AB%E5%8A%A9%E4%BB%96%E5%97%8E/></a></article><article class=post-entry><header class=entry-header><h2>Relation between subdimensions of empathy</h2></header><div class=entry-content><p>常見的同理心的向度 同理心的定義中曾提到，同理心的的定義多元，目前普遍接受同理心是一個包含多元向度（多元能力）的概念（或是能力）。
在這樣子的看法底下，我很自然地認為，這些不同的次向度的能力應該彼此之間是正相關的，因為都被一個共通的潛在構念所影響（圖二左）。
同理心是一個包含多向度的潛在構念或是一個籠統名詞呢？ 不過，Klöckner等人（2022）的研究卻發現，認知上的同理（JSE測量）跟行為上的同理表現（以VR-CODES測量）只有部份相關。更明確地說，JSE的分數高低的差異，只在醫師（實習生）的非口語行為表現上看到差異，口語同理行為則和JSE分數高低無關。
如果這些不同的次向度之間的關係並不是那麼緊密的話，也許同理心比較像是一種籠統的名詞（collective term），將許多不同的能力籠統地括在一起說（圖二右）。這些不同的能力之間不必然有正相關。
如果是這樣子的話，那麼要測量同理心將會更加的困難，因為不同的能力之間如果不一定有關係的話，那麼就沒有辦法只測量其中一項能力而去推估其他能力，這樣就不同測那麼多不同的次能力；相反的，可能要測量許多不同的能力，才可以形成對於同理心能力的印象。
不過，Klöckner等人（2022）只有15個人的資料，樣本數其實很少，這個結果是否是穩定的還需要更多的研究。只是，現有的同理心測量，絕大多數都是自評的認知同理。要有更多探討不同的同理向度間的關係，可能還要有較有效的情緒同理、行為同理的測量工具出現。
參考文獻 Klöckner, C. C., Gerbase, M. W., Nendaz, M., Baroffio, A., & Junod, N. P. (2022). Relationship between self-reported cognitive and behavioural empathy among medical students. Patient Education and Counseling, 105(4), 895–901. https://doi.org/10.1016/j.pec.2021.07.053</p></div><footer class=entry-footer><span title='2022-09-23 00:00:00 +0000 UTC'>September 23, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to Relation between subdimensions of empathy" href=https://juyujeng.github.io/researchlife/blog/research/relation-between-subdimensions-of-empathy/></a></article><article class=post-entry><header class=entry-header><h2>快要中風的養育者</h2></header><div class=entry-content><p>兒子最近得到腸胃炎，一直在拉肚子，特別是喝完奶之後，在乳糖的催化下更是很快地便排了出來。在水便的浸淫之下小屁股很快地便紅了一片。
洗屁股、擦藥，這些都是小事。可怕的是身體狀況所帶來的生活作息的改變。
腸胃炎之後正餐他都吃不太下，大部份靠喝奶補充營養，但是喝奶又會引發拉肚子，導致他得到腸胃炎的這幾天都吃得不好也睡得不好，常常大半夜餓醒。
接連幾天常餓醒之後似乎發現了半夜醒來身邊常常沒有人（廢話…都要等你睡了才有辦法做事啊…），就變成死撐著不睡。
如果是週末我打定主意不做事了倒是沒差，平日晚上就已經沒多少時間可以趕工作了，再經過這樣子的折騰終於可以坐下來打開電腦時常接近午夜12點了。
連續多天下來，睡眠時間不到五小時，感覺像是快中風一般。那個情緒管控的問題明顯到不需要太敏感的自我覺察能力都可以發現。真的是很可怕…
這時看到英國的影集養育者（breeders）第一季的第一集，真的是心有戚戚焉。</p></div><footer class=entry-footer><span title='2022-09-19 00:00:00 +0000 UTC'>September 19, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to 快要中風的養育者" href=https://juyujeng.github.io/researchlife/blog/life/%E5%BF%AB%E8%A6%81%E4%B8%AD%E9%A2%A8%E7%9A%84%E9%A4%8A%E8%82%B2%E8%80%85/></a></article><article class=post-entry><header class=entry-header><h2>同理的困難</h2></header><div class=entry-content><p>同理的困難 make other understand 要做到同理有許多的困難
要先體認到不同的人有不同的角度與觀點，所以會有不同的看法以及預期。 在互動的過程需要理解或是辨認出他人現在的想法或是感受是什麼？ 除了理解到他人現在的想法外，還要理解對方「為什麼」會這麼想。能夠從對方的觀點來思考。才可以知道衝突發生的徵結點。 在可以從對方的觀點來思考之後，你才有辦法選擇以及做出適當反應。因為你要考量到反應之後，對方可能有什麼樣的反應。 每一步，都需要假設性思考、推理的能力。這都不是簡單的事情，以下再進一步說明
知道每個人都是不一樣的 每個人都是不一樣的，每個人都知道，但卻很常忘掉。又或者是說，我們可能很常用其他的方法來解釋其他人和我們的不一樣。
我這邊的不一樣指的是觀點的不同。這個不同有兩種來源，一個是較長期穩定的，基於每個人的成長背景、知識，以及長期和這個世界互動後形成對於世界認識的基模或者是習慣，這些組合成我們看待世界的觀點，影響著我們對於看到的東西所做出的詮釋以及思考歷程。另一個是個人當下的狀態，情緒、身體狀況（例如身體某個部位在痛、昨晚沒睡飽），也會影響當下對世界的看法，以及如果和世界互動的選擇。如果用數學式來看的話，也許會是像這樣吧
$$現在看世界的觀點 = 長期養成的觀點 + 當下的狀態$$
基於上述理由，對於同樣的事情不同的人有著不同的觀點或是感受，是很正常的，因為每個人的背景知識不一樣，當下的身體狀況也可能不一樣。
就像是上圖中的第一格，兩個人從各自不同的觀點來看同一個事物，一邊看起來是9，而另一邊看起來是6。如果雙方不知道彼此的觀點有所不同，那麼很容易都認為對方應該也要看到和自己相同的數字。甚至在發現對方和自己看得不一樣時，會覺得對方一定是錯的，或是對方有問題，才會和自己不一樣。
知道其他人可能和自己不同，才保有雙方繼續互動下去的彈性，否則在發現對方所想的和自己心裡所想的有所不同時，先入為主的認為對方有問題，接下來如果需要雙方達成共識，是接近不可能的事情。
理解或是辨認出他人現在的想法或是感受 在明白每個人的觀點都可能有所不同之後，接下來便是要實際理解到底對方的想法是什麼？畢竟許多人是有經過社會化的，外顯出來的和內心所想的不一定會相同。而在醫療的領域當中，有時候會遇到的情況是病人受限於身體狀況，可能也很難將自己的想法或是感受表達地很清楚。有些時候需要藉由一些身體語言或是引導問答，才有辦法釐清。那才有辦法像圖中第2格一樣，掌握到對方從他的角度看到的數字，是6。
理解對方「為什麼」會這麼想 光是知道對方現在實際的想法或是感受，那只是同理的初步動作。去理解到對方「為什麼」會有這樣的想法或是感受，我認為才算是進到的同理的核心。因為這必須要跳出自己的思考的角度和框架，試著從別人的角度去思考。這需要一些抽象推理能力，或是想像力，才有辦法將自己想像為他人，用另一套看世界的方法來思考。
不過，若是自己曾有過切身經驗，也許會比較簡單，但也可能有反效果。因為若對方和自己在相似的經驗當中是依循著相同的思考模式的話，你可能便可以理解對方為什麼會這麼想。但，每個人在面對相同的事件是可能有不同的反應或是思考模式的。甚至是雖然有相同的反應，但思考模式是完全不同的。
因此，這個部份是同理過程當中相當重要，但也十分困難的一步。若跟對方沒有基本的認識，基本上是很難有辦法去理解到對方為什麼會有這樣子的想法或是感受，只能夠從自己過去的經驗或是知識來去做猜測（要冒著猜錯的風險）。
做出適當反應 如果做到了上一步，那麼便自然而然的可以去做出適當的反應。
就像是圖中第3格，我們理解到對方之所以會將數字看作是6，是因為他從他的角度來看就會是6。而且他似乎沒有注意到數字上有一條底線，顯示9可能才是這個數字的正確答案。
如果知道對方是因為角度的關係，並且可能忽略掉了細微的底線。這時可以邀請對方到我這邊來，並指出底線的存在。又或者是我幫對方將數字掉頭，讓他可以看清楚底線。如果再多考慮到對方受傷行動不便（我應該有畫得很明顯吧？），後面這個做法會是更好的做法。
當還沒有辦法完成上一步，理解對方為什麼會這麼想時，最好不要輕舉妄動，這時最佳的行為準則是想辦法獲得更多的訊息來幫助自己理解對方的思考模式。</p></div><footer class=entry-footer><span title='2022-09-14 00:00:00 +0000 UTC'>September 14, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to 同理的困難" href=https://juyujeng.github.io/researchlife/blog/research/%E5%90%8C%E7%90%86%E7%9A%84%E5%9B%B0%E9%9B%A3/></a></article><article class=post-entry><header class=entry-header><h2>OT學生的同理等級：文獻閱讀心得 2</h2></header><div class=entry-content><p>OT學生的同理等級：文獻閱讀心得 2 繼昨天看到的那篇Brown等人（2010）的文章後，今天看到一篇非常類似的文章。Serrada-Tejeda等人（2022）在西班牙的大學中等於是重覆了這個研究，他們廣發量表（[[Jefferson Scale of Empathy]]以及[[interpersonal reactivity inventory]]，這兩個都是常見的自評同理心量表），同樣以橫斷性研究的方式來檢驗大學四年的OT訓練是否會影響OT學生們的自評同理程度。
不過這次Serrada-Tejeda等人有221名有效樣本，比起Brown他們只有92位真得是好太多了。而且四個年級的學生都有（一到四年級分別有71、54、46、50人）。不過大部份仍然是女性（88.2％）。
Serrada-Tejeda他們的結果和Brown一樣，四個年級的學生的自評同理心並沒有差異，而且得分和過去文獻中回報的比起來，這個大學的OT學生的自評同理程度算是高的。
而與Brown等人的研究不同的是，這次Serrada-Tejeda他們有發現性別的差異，女性自評同理程度比較高。但這並不是新聞，許多其他的研究其實都有發現女性的同理心分數通常比男性高。Brown等人的研究沒有得到這個結果應該是人太少吧。
這篇研究的分析結果比起Brown他們寫得再多了一點點，至少有附上表格，可以比較清楚的看到四個年級的得分。
心得與發想 連續看到兩篇OT學生的學校訓練年資與自評同理程度無關的文章，會有這種結果其實有三種可能。第一個可能是這些訓練無助於同理心的增長；而第二種可能是學校訓練有用，但使用的測量工具測量不到學校訓練所增進的同理心；而第三種就是訓練無助同理心，而測量工具也測不到同理心。
第一種可能其實挺有可能的，畢竟主要的學校訓練和治療相關的專業知識比較有關係，同理心相關技巧不一定有在課程之中，文獻當中也沒有詳細說明。
第二種可能則是工具的問題，雖然說兩篇研究都是用目前常見的同理心自評量表，但其實仔細看題目，實在是會有點問號，像是JSE當中有一題：
I believe that emotion has no place in the treatment of medical illness
這個像是在測量醫事人員本身的信念，而不是能力。或是像另一題，同樣對於是否能測量到同理心打一個問號：
I have a good sense of humor that I think contributes to a better clinical outcome
因此，如果有更適合的工具可以使用的話，也許可以試著在台灣也來做一個調查試試看。而且這個調查若同時可以比較醫學院的其他科系，同時看看不同的醫學院內科系訓練對於學生們的同理心是否有所幫助，會是很有趣的議題。因為目前國外就已經有用JSE在不同科系進行過研究，有的有發現會隨著訓練年資同理心上升，而有的發現隨著訓練年資同理心下降。也許不同科系的訓練真的有所不同。例如工作上面和病人接觸時間較短的醫技系、藥學系的訓練，也許真的就和其他跟病人接觸時間較長的科系如護理系、醫學系，所強調同理心的程度不一樣，訓練也就不一樣。
文獻 Brown, T., Williams, B., Boyle, M., Molloy, A., McKenna, L., Molloy, L., & Lewis, B. (2010). Levels of empathy in undergraduate occupational therapy students....</p></div><footer class=entry-footer><span title='2022-09-01 00:00:00 +0000 UTC'>September 1, 2022</span>&nbsp;·&nbsp;Ju</footer><a class=entry-link aria-label="post link to OT學生的同理等級：文獻閱讀心得 2" href=https://juyujeng.github.io/researchlife/blog/research/ot%E5%AD%B8%E7%94%9F%E7%9A%84%E5%90%8C%E7%90%86%E7%AD%89%E7%B4%9A%E6%96%87%E7%8D%BB%E9%96%B1%E8%AE%80%E5%BF%83%E5%BE%97-2/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://juyujeng.github.io/researchlife/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>contact me @ <a href="mailto: juyujeng@gmail.com">juyujeng@gmail.com</a></span><br><span>&copy; 2022 <a href=https://juyujeng.github.io/researchlife/>Ju' Research and Life</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>